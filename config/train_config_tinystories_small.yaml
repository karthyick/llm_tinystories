# TinyStories Optimized Config - 33M Model (RECOMMENDED)
# Based on Config 2 with small safety improvements

model:
  vocab_size: 32000
  d_model: 448
  n_layers: 7
  n_heads: 7
  d_ffn: 1344
  max_seq_len: 512        # âœ… Perfect for TinyStories
  dropout: 0.1
  rope_percentage: 0.5
  rope_base: 10000
  rms_norm_eps: 1e-6
  tie_embeddings: true
  use_flash_attention: true

training:
  batch_size: 64
  num_epochs: 15      # Official paper uses 20, we use 15 for faster experimentation
  gradient_accumulation_steps: 20 
  
  use_bf16: true
  use_gradient_checkpointing: false
  use_compile: false
  
  eval_steps: 200
  save_steps: 500
  logging_steps: 10
  
  show_progress: true
  use_wandb: false          # ðŸ”§ ENABLE for tracking
  output_dir: "./checkpoints"

optimizer:
  learning_rate: 5e-4      # âœ… Official
  adam_beta1: 0.9          # âœ… Official
  adam_beta2: 0.95         # âœ… Official
  adam_epsilon: 1e-8
  weight_decay: 0.1        # âœ… Official
  max_grad_norm: 1.0

scheduler:
  type: constant           # âœ… Official (with safety warmup)
  warmup_steps: 100        # ðŸ”§ ADD tiny warmup for stability
  # Note: Even with "constant" type, 100 steps warmup helps
  # Then maintains 5e-4 for rest of training

data:
  dataset: "tinystories"
  tokenizer_path: "./tokenizer/wikimini_32k"
  cache_dir: "./data/cache"
  max_seq_len: 512         # âœ… Perfect for dataset
  
  check_quality: true
  quality_sample_size: 10000
  quality_strict: false

hardware:
  device: "cuda"
  num_gpus: 1
