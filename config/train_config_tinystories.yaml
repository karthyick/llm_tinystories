# TinyStories Training Config for RTX 5090
# Clean, simple dataset perfect for quick training and testing
# Dataset: ~2.1M stories, ~1GB, excellent English quality

model:
  vocab_size: 32000
  d_model: 768
  n_layers: 12
  n_heads: 12
  d_ffn: 2048
  max_seq_len: 1024  # TinyStories has shorter sequences - 1024 is perfect
  dropout: 0.1
  rope_percentage: 0.5
  rope_base: 10000
  rms_norm_eps: 1e-6
  tie_embeddings: true
  use_flash_attention: true  # Works great with batch_size=8

training:
  # Optimized batch configuration
  batch_size: 8  # Sweet spot for RTX 5090
  num_epochs: 15  # Official paper uses 20, we use 15 for faster experimentation

  # Gradient accumulation for effective batch size
  gradient_accumulation_steps: 64  # Effective batch = 8 × 64 × 1024 = 524,288 tokens

  # Mixed precision
  use_bf16: true

  # Gradient checkpointing - Disabled (we have 32GB VRAM)
  use_gradient_checkpointing: false

  # Optimization
  use_compile: false  # Disabled on Windows

  # Evaluation frequency
  eval_steps: 200
  save_steps: 500
  logging_steps: 10

  show_progress: true
  use_wandb: false
  output_dir: "./checkpoints"

optimizer:
  learning_rate: 6e-4
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-8
  weight_decay: 0.1
  max_grad_norm: 1.0

scheduler:
  type: cosine
  warmup_steps: 500
  min_lr: 6e-5

data:
  dataset: "tinystories"  # NEW: Use TinyStories instead of WikiText
  tokenizer_path: "./tokenizer/wikimini_32k"
  cache_dir: "./data/cache"
  max_seq_len: 1024  # Must match model.max_seq_len

  # Data quality checks (recommended!)
  check_quality: true  # Run quality checks before training
  quality_sample_size: 10000  # Number of samples to check (null for all)
  quality_strict: false  # If true, fail on any issues; if false, allow warnings

hardware:
  device: "cuda"
  num_gpus: 1
