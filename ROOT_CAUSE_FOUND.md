# ğŸ¯ ROOT CAUSE ANALYSIS - COMPLETE

## Critical Finding: **DATA PIPELINE IS PERFECT!**

After comprehensive testing of the entire data pipeline, articles are **PRESERVED** at every stage:

### âœ… Complete Data Pipeline Verification:

| Stage | Article % | Status |
|-------|-----------|--------|
| **Raw TinyStories data** | 9.93% of words | âœ… NORMAL |
| **After tokenization** | 6.23% of tokens | âœ… NORMAL |
| **TinyStoriesDataset class** | 2.26% 'a', 3.51% 'the' | âœ… NORMAL |
| **Sequence packing** | 5.9-8.8% per sequence | âœ… NORMAL |
| **Training batches** | 5.9-8.8% per batch | âœ… NORMAL |

**Conclusion:** The training data reaching your model contains articles correctly!

---

## ğŸ” What This Means

Your model is being trained on data with proper articles, but it's not generating them. This indicates:

### **The Problem is in the MODEL or TRAINING, not the DATA**

Given your observations:
- âœ… Good perplexity (PPL 7.37)
- âŒ Broken generation (missing "a", "the", produces "todlers")
- âŒ Low probability for articles in validation

### Possible Root Causes:

1. **Model Architecture Issue**
   - Model may not have capacity to learn article placement rules
   - Attention patterns might not capture the syntactic patterns for articles
   - Position embeddings might not encode the right context

2. **Training Process Issue**
   - Loss function averages over all tokens equally
   - Article errors (low frequency) get averaged out by other tokens
   - Model optimizes for common words, neglects grammatical function words

3. **Generation Issue**
   - Sampling/decoding might have bias against low-frequency tokens
   - Temperature/top-k/top-p might filter out articles
   - Greedy decoding might prefer other tokens

4. **Training Data Imbalance**
   - While articles are present (6%), they're less frequent than content words
   - Model might learn to predict "little" (high frequency) instead of "a little"
   - Articles depend heavily on context, harder to learn than content words

---

## ğŸ“ Why Good PPL but Bad Generation?

**Perplexity measures average prediction quality**, not generation quality:

```
Your model might predict:
âœ… "little" with high probability (correct!)
âœ… "girl" with high probability (correct!)
âŒ " a" with low probability (WRONG!)

Average perplexity: GOOD
But generation: "there was little girl" âŒ
```

The model learned **content words** well but failed to learn **grammatical function words** (articles).

---

## ğŸ”¬ Next Steps: Diagnose the Model

Now that we know the data is perfect, we need to check what the model learned:

### 1. **Check Model's Article Predictions in Context**

Run this test:
```python
prompt = "Once upon a time there was"
# Model should predict " a" or " the" next
# Check: Does it even consider articles? What's their probability?
```

### 2. **Analyze Model's Attention Patterns**

- Do attention heads focus on the right context for article placement?
- Example: "there was ___ little girl" - does attention look at "little girl" to decide article?

### 3. **Check Training Loss on Articles Specifically**

- What's the loss for predicting article tokens vs other tokens?
- Are article predictions getting worse during training?

### 4. **Test Generation with Different Sampling**

Your validation used:
- Temperature 0.8
- Top-k 50

Try:
- Temperature 1.0 (less aggressive)
- No top-k filtering
- Nucleus (top-p) sampling

### 5. **Check if Model Ever Learned Articles**

Look at early checkpoints - did the model ever generate articles correctly, then forget them?

---

## ğŸ› ï¸ Potential Fixes

Based on the root cause, try:

### A. **Increase Article Learning Signal**

```python
# Weight loss more heavily for article tokens
article_token_ids = {262, 264, 389}  # ' a', ' the', ' an'
loss_weights = torch.ones_like(labels)
loss_weights[labels in article_token_ids] *= 5.0  # 5x weight
```

### B. **Add Article-Specific Metrics During Training**

Track article prediction accuracy separately:
```python
article_acc = accuracy for tokens in {262, 264, 389}
```

### C. **Curriculum Learning**

Train first on sequences rich in articles, then gradually add variety.

### D. **Architecture Changes**

- Increase model capacity (more layers/heads)
- Add special "function word" attention heads
- Use mixture of experts for different token types

### E. **Data Augmentation**

Create synthetic examples emphasizing article patterns:
```
"a cat" â†’ " a cat"
"the dog" â†’ " the dog"
```

---

## ğŸ“Š Quick Test You Can Run Now

Create this simple test to see what your model predicts:

```python
import torch
from src.model.transformer_block import WikiMiniModel
from src.data.tokenizer import load_tokenizer

# Load model and tokenizer
tokenizer = load_tokenizer('./tokenizer/wikimini_32k')
checkpoint = torch.load('checkpoints_wikimini/best_model.pth')
model = WikiMiniModel(checkpoint['config']['model'])
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Test cases where articles are required
test_prompts = [
    "Once upon a time there was",  # Should predict ' a' or ' the'
    "There was",                    # Should predict ' a' or ' the'
    "She saw",                      # Should predict ' a' or ' the'
]

for prompt in test_prompts:
    tokens = tokenizer.encode(prompt)
    input_ids = torch.tensor([tokens])

    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs['logits'][0, -1, :]  # Last token predictions
        probs = torch.softmax(logits, dim=0)

    # Check article probabilities
    print(f"\nPrompt: '{prompt}'")
    print(f"  ' a' (262):   {probs[262]:.4%} (rank #{(probs > probs[262]).sum().item()})")
    print(f"  ' the' (264): {probs[264]:.4%} (rank #{(probs > probs[264]).sum().item()})")

    # Show top 5 predictions
    top5 = torch.topk(probs, 5)
    print(f"  Top 5:")
    for i, (prob, idx) in enumerate(zip(top5.values, top5.indices)):
        token_text = tokenizer.decode([idx.item()])
        print(f"    {i+1}. '{token_text}' - {prob:.4%}")
```

This will show you EXACTLY what the model learned about articles in context!

---

## ğŸ“ Summary

âœ… **Data is perfect** - articles present at all stages
âŒ **Model didn't learn articles** - despite seeing them in training
ğŸ¯ **Focus on model/training** - not data pipeline
ğŸ”¬ **Next: diagnose what model learned** - check predictions in context
